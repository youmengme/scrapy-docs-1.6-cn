.. _intro-overview:

==================
Scrapy 一目了然
==================

Scrapy是一种用于抓取网站和提取结构化数据的应用程序框架，可用于广泛的有用应用程序，如数据挖掘，信息处理或历史存档。
尽管Scrapy最初是为网络抓取而设计的，但它也可以用于使用API (例如 `Amazon Associates Web Services`_) 
或作为通用网络爬虫来提取数据。


浏览示例爬虫
=================================

为了向您展示Scrapy带来的内容，我们将以最简单的方式运行爬虫，向您介绍Scrapy Spider的示例。

这是一个爬虫的代码，它从网页 http://quotes.toscrape.com 上抓取名言, 并按照一下分页::

    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = 'quotes'
        start_urls = [
            'http://quotes.toscrape.com/tag/humor/',
        ]

        def parse(self, response):
            for quote in response.css('div.quote'):
                yield {
                    'text': quote.css('span.text::text').get(),
                    'author': quote.xpath('span/small/text()').get(),
                }

            next_page = response.css('li.next a::attr("href")').get()
            if next_page is not None:
                yield response.follow(next_page, self.parse)


将它放在一个文本文件中，将其命名为类似 ``quotes_spider.py`` 的名称
并使用 :command:`runspider` 命令运行::

    scrapy runspider quotes_spider.py -o quotes.json


完成后，您将在 ``quotes.json`` 文件中包含JSON格式的列表, 其中包含文本和作者，如下所示（为了更好的可读性，此处重新格式化）::

    [{
        "author": "Jane Austen",
        "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"
    },
    {
        "author": "Groucho Marx",
        "text": "\u201cOutside of a dog, a book is man's best friend. Inside of a dog it's too dark to read.\u201d"
    },
    {
        "author": "Steve Martin",
        "text": "\u201cA day without sunshine is like, you know, night.\u201d"
    },
    ...]


刚刚发生了什么？
-------------------

当您运行命令 ``scrapy runspider quotes_spider.py``, Scrapy在其中查找Spider定义并通过其爬虫引擎运行它。

通过向 ``start_urls`` 属性中定义的URL发出请求 (在这种情况下，只有 humor 类别的 URL )
开始爬行，并调用默认回调方法 ``parse``, 将响应对象作为参数传递。在 ``parse`` 回调中，我们使用CSS Selector循环遍历quote元素, 使用 yield 提取名言的文本和作者生成Python dict,查找指向下一页的链接，并使用与 ``parse`` 回调相同的方法安排另一个请求。

在这里，您会注意到Scrapy的一个主要优点：请求是
:ref:`异步调度和处理的 <topics-architecture>`.  这意味着Scrapy不需要等待请求完成和处理，它可以在此期间发送另一个请求或执行其他操作。这也意味着其他请求可以继续，即使一些请求失败或处理它时出现错误。

虽然这使得您能够执行非常快速的抓取（同时发送多个并发请求，容错方式），Scrapy 也允许通过 :ref:`一些设置
<topics-settings-ref>` 来控制爬取的行为。. 您可以执行各种操作，例如在每个请求之间设置下载延迟，限制每个域或每个IP的并发请求数量，甚至 :ref:`使用自动限制扩展 <topics-autothrottle>` 来自动计算这些延迟。

.. note::

    这是使用 :ref:`feed 导出 <topics-feed-exports>` 来生成JSON文件，您可以轻松地更改导出格式（例如XML或CSV）或存储后端（例如FTP或 `Amazon S3`_ ).  您还可以编写
    :ref:`item pipeline <topics-item-pipeline>` 以将item存储在数据库中。


.. _topics-whatelse:

还有什么？
==========

您已经了解了如何使用Scrapy从网站中提取和存储项目，但这只是表面。Scrapy提供了许多强大的功能，可以轻松高效地进行抓取，例如：

* 内置支持使用扩展的CSS选择器和XPath表达式从HTML/XML源 :ref:`选择和提取 <topics-selectors>` 数据，以及使用正则表达式提取的帮助方法。

* 一个 :ref:`交互式shell控制台 <topics-shell>` （支持IPython） 用于尝试使用CSS和XPath表达式来抓取数据，在编写或调试爬虫时非常有用。

* 内置支持以多种格式（JSON，CSV，XML）:ref:`生成Feed导出 <topics-feed-exports>` 并将它们存储在多个后端（FTP，S3，本地文件系统）

* 强大的编码支持和自动检测，用于处理外部，非标准和损坏的编码声明。

* :ref:`强大的可扩展性支持 <extending-scrapy>`, 允许您使用 :ref:`信号 <topics-signals>` 和定义良好的API（中间件， :ref:`扩展 <topics-extensions>`, 和
  :ref:`管道 <topics-item-pipeline>` ）插入您自己的功能。.

* 广泛的内置扩展和中间件处理：

  - cookies 和会话处理
  - HTTP功能，如压缩，身份验证，缓存
  - user-agent 模拟
  - robots.txt
  - 爬取深度限制
  - 更多

* 内置 :ref:`Telnet 控制台 <topics-telnetconsole>` 用于连接到Scrapy进程中运行的Python控制台, 使您可以查看并且调试爬虫

* 此外，其他好东西，如可重复使用的 spiders，从 `Sitemaps`_ 和
  XML/CSV 源中抓取网站，, 一个媒体管道 ，用于 :ref:`自动下载图像
  <topics-media-pipeline>` 与抓取项目相关联的图像 (或任何其他媒体)， 缓存DNS解析器等等！
  
下一步是什么？
============

接下来的步骤是 :ref:`安装 Scrapy <intro-install>`,
:ref:`按照教程 <intro-tutorial>` 学习如何创建一个成熟的Scrapy项目并 `加入社区`_. 感谢您的关注！

.. _加入社区: https://scrapy.org/community/
.. _web scraping: https://en.wikipedia.org/wiki/Web_scraping
.. _Amazon Associates Web Services: https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html
.. _Amazon S3: https://aws.amazon.com/s3/
.. _Sitemaps: https://www.sitemaps.org/index.html
