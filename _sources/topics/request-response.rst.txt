.. _topics-request-response:

======================
请求与响应
======================

.. module:: scrapy.http
   :synopsis: Request and Response classes

Scrapy 使用 :class:`Request` 和 :class:`Response` 对象来抓取网站。

通常, :class:`Request` 对象在Spider中生成并通过系统直到它们到达下载器，下载器执行请求并返回一个 :class:`Response` 对象，该对象返回发出请求的蜘蛛。

:class:`Request` 和 :class:`Response` 类都有添加基类中没有的功能的子类。这些在 :ref:`topics-request-response-ref-request-subclasses` 和
:ref:`topics-request-response-ref-response-subclasses`.


Request 对象
===============

.. class:: Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback, flags])

    :class:`Request`  对象表示HTTP请求，它通常在Spider中生成并由下载器执行，从而生成  :class:`Response`.

    :param url:  此次请求的URL
    :type url: string

    :param callback: 将使用此请求的response（一旦其下载）作为其第一个参数来调用的函数。有关更多信息，请参阅下面的 :ref:`topics-request-response-ref-request-callback-arguments`。
       如果请求没有指定回调，将使用spider的
       :meth:`~scrapy.spiders.Spider.parse` 方法。请注意，如果在处理期间引发异常，则会调用errback。

    :type callback: callable

    :param method: 此请求的HTTP方法。默认为 ``'GET'``.
    :type method: string

    :param meta: :attr:`Request.meta` 属性的初始值。如果给定，则此参数中传递的dict将被浅层复制。
    :type meta: dict

    :param body: 请求体。如果传递 ``unicode`` 那么它使用传递的编码（默认为 ``utf-8`` )
      编码为``str``. 如果没有给出
      ``body`` 则存储一个空字符串。无论此参数的类型，存储的最终值将是一个 ``str`` (不会是 ``unicode`` 或 ``None``).
    :type body: str or unicode

    :param headers: 此请求的 header。 dict值可以是字符串（对于单值header）或列表（对于多值header）。如果
       ``None`` 作为值传递，则不会发送HTTP header 。
    :type headers: dict

    :param cookies: 请求cookie。这些可以以两种形式发送。

        1. 使用 dict::

            request_with_cookies = Request(url="http://www.example.com",
                                           cookies={'currency': 'USD', 'country': 'UY'})

        2. 使用带有dict的lis::

            request_with_cookies = Request(url="http://www.example.com",
                                           cookies=[{'name': 'currency',
                                                    'value': 'USD',
                                                    'domain': 'example.com',
                                                    'path': '/currency'}])

        后一种形式允许定制cookie的 ``domain`` 和 ``path``
        属性。这只有在保存Cookie用于以后的请求时才有用。

        .. reqmeta:: dont_merge_cookies

       当某些网站返回Cookie（在response中）时，这些Cookie会存储在该域的Cookie中，并在将来的请求中携带上该Cookie再次发送。这是任何普通网络浏览器的典型行为。但是，如果由于某些原因，您想要避免与现有Cookie合并，您可以通过在 :attr:`Request.meta` 中将 ``dont_merge_cookies`` 键设置为True来指示Scrapy执行此操作。

        不合并Cookie的请求示例::

            request_with_cookies = Request(url="http://www.example.com",
                                           cookies={'currency': 'USD', 'country': 'UY'},
                                           meta={'dont_merge_cookies': True})

        有关详细信息，请参阅 :ref:`cookies-mw`.
    :type cookies: dict or list

    :param encoding: 此请求的编码（默认为 ``'utf-8'``).
       此编码将用于对URL进行百分比编码，并将body转换为 ``str`` (如果给出为 ``unicode``).
    :type encoding: string

    :param priority: 此请求的优先级（默认为 ``0``). 调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以表示相对低优先级。
    :type priority: int

    :param dont_filter: 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用此选项。小心使用它，否则你会进入爬行循环。默认为 ``False``.
    :type dont_filter: boolean

    :param errback: 如果在处理请求时引发任何异常，将调用的函数。这包括失败的404 HTTP错误等页面。它接收一个 `Twisted Failure`_ 实例作为第一个参数。有关更多信息，请参阅下面的 :ref:`topics-request-response-ref-errbacks` below.
    :type errback: callable

    :param flags:  发送到请求的标志，可用于日志记录或类似目的。
    :type flags: list

    .. attribute:: Request.url

       包含此请求的URL的字符串。请记住，此属性包含转义的URL，因此它可能与构造函数中传递的URL不同。

        此属性为只读。要更改请求的URL，请使用
        :meth:`replace`.

    .. attribute:: Request.method

       表示请求中的HTTP方式的字符串。这保证是大写的。示例: ``"GET"``, ``"POST"``, ``"PUT"`` 等

    .. attribute:: Request.headers

        类似于字典的对象，包含请求标头。
    .. attribute:: Request.body

        包含请求正文的str。

        该属性是只读的，要更改请求使用的正文
        :meth:`replace`.

    .. attribute:: Request.meta

        包含此请求的任意元数据的字典。对于新的请求，此dict为空，并且通常由不同的Scrapy组件（扩展，中间件等）填充。因此，此dict中包含的数据取决于您启用的扩展。
        有关Scrapy识别的特殊元键列表，请参阅 :ref:`topics-request-meta` for a list of special meta keys.

        当使用
        ``copy()`` 或 ``replace()`` 方法克隆请求时，此dict是 `shallow copied` 并且也可以在您的爬虫中从 ``response.meta`` 属性访问。

    .. _shallow copied: https://docs.python.org/2/library/copy.html

    .. method:: Request.copy()

       返回一个新请求，该请求是此请求的副本。另请参阅:
       :ref:`topics-request-response-ref-request-callback-arguments`.

    .. method:: Request.replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])

       返回具有相同成员的Request对象，但通过指定的任何关键字参数给出新值的成员除外。 :attr:`Request.meta` 默认情况下复制该属性（除非参数中给出了新值 ``meta`` ). 另请参阅将
       :ref:`topics-request-response-ref-request-callback-arguments`.

.. _topics-request-response-ref-request-callback-arguments:

将附加数据传递给回调函数
---------------------------------------------

请求的回调是在下载该请求的响应时将被调用的函数。将使用下载的 :class:`Response` 对象作为其第一个参数调用回调函数。

示例::

    def parse_page1(self, response):
        return scrapy.Request("http://www.example.com/some_page.html",
                              callback=self.parse_page2)

    def parse_page2(self, response):
        # this would log http://www.example.com/some_page.html
        self.logger.info("Visited %s", response.url)

在某些情况下，您可能有兴趣将参数传递给那些回调函数，以便稍后在第二个回调中接收参数。您可以使用该 :attr:`Request.meta` 属性.

以下是如何使用此机制传递项目以填充不同页面中的不同字段的示例::

    def parse_page1(self, response):
        item = MyItem()
        item['main_url'] = response.url
        request = scrapy.Request("http://www.example.com/some_page.html",
                                 callback=self.parse_page2)
        request.meta['item'] = item
        yield request

    def parse_page2(self, response):
        item = response.meta['item']
        item['other_url'] = response.url
        yield item


.. _topics-request-response-ref-errbacks:

使用errbacks来捕获请求处理中的异常
--------------------------------------------------------

请求的错误返回是在处理异常时将调用的函数。

它接收 `Twisted Failure`_ 实例作为第一个参数，可用于跟踪连接建立超时，DNS错误等。

这是蜘蛛记录所有错误并在需要时捕获一些特定错误的示例::

    import scrapy

    from scrapy.spidermiddlewares.httperror import HttpError
    from twisted.internet.error import DNSLookupError
    from twisted.internet.error import TimeoutError, TCPTimedOutError

    class ErrbackSpider(scrapy.Spider):
        name = "errback_example"
        start_urls = [
            "http://www.httpbin.org/",              # HTTP 200 expected
            "http://www.httpbin.org/status/404",    # Not found error
            "http://www.httpbin.org/status/500",    # server issue
            "http://www.httpbin.org:12345/",        # non-responding host, timeout expected
            "http://www.httphttpbinbin.org/",       # DNS error expected
        ]

        def start_requests(self):
            for u in self.start_urls:
                yield scrapy.Request(u, callback=self.parse_httpbin,
                                        errback=self.errback_httpbin,
                                        dont_filter=True)

        def parse_httpbin(self, response):
            self.logger.info('Got successful response from {}'.format(response.url))
            # do something useful here...

        def errback_httpbin(self, failure):
            # log all failures
            self.logger.error(repr(failure))

            # in case you want to do something special for some errors,
            # you may need the failure's type:

            if failure.check(HttpError):
                # these exceptions come from HttpError spider middleware
                # you can get the non-200 response
                response = failure.value.response
                self.logger.error('HttpError on %s', response.url)

            elif failure.check(DNSLookupError):
                # this is the original request
                request = failure.request
                self.logger.error('DNSLookupError on %s', request.url)

            elif failure.check(TimeoutError, TCPTimedOutError):
                request = failure.request
                self.logger.error('TimeoutError on %s', request.url)

.. _topics-request-meta:

Request.meta special keys
=========================

:attr:`Request.meta` 属性可以包含任意数据，但Scrapy及其内置扩展可识别一些特殊键。

那些是:

* :reqmeta:`dont_redirect`
* :reqmeta:`dont_retry`
* :reqmeta:`handle_httpstatus_list`
* :reqmeta:`handle_httpstatus_all`
* :reqmeta:`dont_merge_cookies`
* :reqmeta:`cookiejar`
* :reqmeta:`dont_cache`
* :reqmeta:`redirect_urls`
* :reqmeta:`bindaddress`
* :reqmeta:`dont_obey_robotstxt`
* :reqmeta:`download_timeout`
* :reqmeta:`download_maxsize`
* :reqmeta:`download_latency`
* :reqmeta:`download_fail_on_dataloss`
* :reqmeta:`proxy`
* ``ftp_user`` (详情请参阅参考资料 :setting:`FTP_USER` )
* ``ftp_password`` (详情请参阅参考资料 :setting:`FTP_PASSWORD` )
* :reqmeta:`referrer_policy`
* :reqmeta:`max_retry_times`

.. reqmeta:: bindaddress

bindaddress
-----------

用于执行请求的传出IP地址的IP。

.. reqmeta:: download_timeout

download_timeout
----------------

下载程序在超时之前等待的时间（以秒为单位）。另见: :setting:`DOWNLOAD_TIMEOUT`.

.. reqmeta:: download_latency

download_latency
----------------

自请求启动以来，获取响应所花费的时间，即通过网络发送的HTTP消息。只有在下载响应后，此元键才可用。虽然大多数其他元键用于控制Scrapy行为，但这个元素应该是只读的。

.. reqmeta:: download_fail_on_dataloss

download_fail_on_dataloss
-------------------------

是否在破坏的回复中失败。请参见:
:setting:`DOWNLOAD_FAIL_ON_DATALOSS`.

.. reqmeta:: max_retry_times

max_retry_times
---------------

元键用于每个请求的设置重试次数。初始化时
:reqmeta:`max_retry_times` 元键优先于
:setting:`RETRY_TIMES` 设置.

.. _topics-request-response-ref-request-subclasses:

Request 子类
==================

这是内置 :class:`Request` 子类的列表。您还可以将其子类化以实现自己的自定义功能。

FormRequest 对象
-------------------

FormRequest 类扩展了基础 :class:`Request` 具有处理HTML表单的功能。它使用 `lxml.html forms`_  来预先填充表单字段，其中包含来自 :class:`Response` 对象的表单数据。

.. _lxml.html forms: http://lxml.de/lxmlhtml.html#forms

.. class:: FormRequest(url, [formdata, ...])

    :class:`FormRequest` 类增加了新的参数构造函数。其余参数与 :class:`Request` 类相同，此处未记录。

    :param formdata: 是包含HTML表单数据的字典（或（key，value）元组的可迭代的），它将被url编码并分配给请求的主体。
    :type formdata: dict or iterable of tuples

    The :class:`FormRequest` 对象支持除标准以下类方法 :class:`Request` 方法:

    .. classmethod:: FormRequest.from_response(response, [formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, ...])

       返回一个新 :class:`FormRequest` 对象，其表单字段值预先填充 ``<form>`` 在给定响应中包含的HTML 元素中找到的值。有关示例，请参阅
       :ref:`topics-request-response-ref-request-userlogin`.

       默认情况下，策略是在任何看起来可单击的窗体控件上自动模拟单击，如 ``<input type="submit">``. 尽管这非常方便，而且常常是所需的行为，但有时它可能会导致难以调试的问题。例如，当处理使用javascript填充和/或提交的表单时， :meth:`from_response` 行为”的默认值可能不是最合适的。要禁用此行为，可以将
       ``dont_click`` 参数设置为 ``True``. 此外，如果要更改单击的控件（而不是禁用它），也可以使用
       ``clickdata`` 参数.

       .. caution:: 将此方法与选项值中包含前导或尾随空格的select元素一起使用将不起作用，因为
          `lxml中的错误`_应该在lxml 3.8及更高版本中修复。
          
       :param response: 包含HTML表单的响应，该表单将用于预填充表单字段
       :type response: :class:`Response` object

       :param formname: 如果给定，将使用name属性设置为此值的表单。
       :type formname: string

       :param formid: 如果给定，将使用id属性设置为此值的表单。
       :type formid: string

       :param formxpath: 如果给定，将使用与xpath匹配的第一个表单。
       :type formxpath: string

       :param formcss: 如果给定，将使用与css选择器匹配的第一个表单。
       :type formcss: string

       :param formnumber: 如果给定，将使用与css选择器匹配的第一个表单。 ``0``.
       :type formnumber: integer

       :param formdata: 要在表单数据中覆盖的字段。如果响应 ``<form>`` 元素中已存在某个字段，则该值将被此参数中传递的值覆盖。如果此参数中传递的值为 ``None``, 则该字段将不会包含在请求中，即使该字段存在于响应 ``<form>`` 元素中也是如此。
       :type formdata: dict

       :param clickdata: 用于查找单击控件的属性。如果没有给出，将提交表单数据，模拟第一个可点击元素的点击。除了html属性之外，还可以通过属性，通过其相对于表单内其他可提交输入的从零开始的索引来标识控件 ``nr`` 属性.
       :type clickdata: dict

       :param dont_click: 如果为True，将提交表单数据而不单击任何元素。
       :type dont_click: boolean

       此类方法的其他参数将直接传递给
       :class:`FormRequest` 构造函数。

       .. versionadded:: 0.10.3
          The ``formname`` parameter.

       .. versionadded:: 0.17
          The ``formxpath`` parameter.

       .. versionadded:: 1.1.0
          The ``formcss`` parameter.

       .. versionadded:: 1.1.0
          The ``formid`` parameter.

Request 使用示例
----------------------

使用FormRequest通过HTTP POST发送数据
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

如果您想在蜘蛛中模拟HTML表单POST并发送几个键值字段，您可以返回一个 :class:`FormRequest` object (from your
spider) like this::

   return [FormRequest(url="http://www.example.com/post/action",
                       formdata={'name': 'John Doe', 'age': '27'},
                       callback=self.after_post)]

.. _topics-request-response-ref-request-userlogin:

使用 FormRequest.from_response（）来模拟用户登录
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

网站通常通过 ``<inputtype="hidden">`` 元素提供预先填充的表单字段，例如会话相关数据或身份验证395令牌（用于登录页面）。 在抓取时，您将希望这些字段自动预填充396并仅覆盖其中的几个，例如397用户名和密码。 您可以将此 :meth:`FormRequest.from_response`
方法用于此作业。这是一个使用它的示例蜘蛛::


    import scrapy

    def authentication_failed(response):
        # TODO: Check the contents of the response and return True if it failed
        # or False if it succeeded.
        pass

    class LoginSpider(scrapy.Spider):
        name = 'example.com'
        start_urls = ['http://www.example.com/users/login.php']

        def parse(self, response):
            return scrapy.FormRequest.from_response(
                response,
                formdata={'username': 'john', 'password': 'secret'},
                callback=self.after_login
            )

        def after_login(self, response):
            if authentication_failed(response):
                self.logger.error("Login failed")
                return

            # continue scraping with authenticated session...


Response 对象
================

.. class:: Response(url, [status=200, headers=None, body=b'', flags=None, request=None])

    :class:`Response` 对象表示的HTTP响应，这通常是下载（由下载器）并馈送到蜘蛛进行处理。

    :param url: 响应的URL
    :type url: string

    :param status: 响应的HTTP状态。默认为 ``200``.
    :type status: integer

    :param headers: 此响应的标头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。
    :type headers: dict

    :param body: 响应主体。要将解码后的文本作为str（Python 2中的unicode）访问，您可以使用 ``response.text`` 来自编码感知的:ref:`Response subclass <topics-request-response-ref-response-subclasses>`,
       例如 :class:`TextResponse`.
    :type body: bytes

    :param flags: 是包含
       :attr:`Response.flags` 属性初始值的列表 。如果给定，列表将被浅层复制。
    :type flags: list

    :param request:  :attr:`Response.request` 属性的初始值。这表示 :class:`Request` 生成此响应。
    :type request: :class:`Request` object

    .. attribute:: Response.url

        包含响应URL的字符串。

        此属性是只读的。要更改响应的URL，请使用
        :meth:`replace`.

    .. attribute:: Response.status

        表示响应的HTTP状态的整数。示例: ``200``,
        ``404``.

    .. attribute:: Response.headers

       类似字典的对象，包含响应头。可以使用 :meth:`get` 以返回具有指定名称的第一个标头值来访问值，或者 :meth:`getlist` 返回具有指定名称的所有标头值。例如，此调用将为您提供标题中的所有Cookie::

            response.headers.getlist('Set-Cookie')

    .. attribute:: Response.body

        这个回复的正文。请记住，Response.body始终是一个字节对象。如果您想使用unicode版本
        :attr:`TextResponse.text` (仅在:class:`TextResponse`
        和子类中可用).

        该属性是只读的。要更改响应使用的正文
        :meth:`replace`.

    .. attribute:: Response.request

        The :class:`Request` 生成此响应的对象。在响应和请求通过所有 :ref:`Downloader Middlewares <topics-downloader-middleware>`.
        之后，在Scrapy引擎中分配此属性。特别是，这意味着:

        - HTTP重定向将导致原始请求（重定向前的URL）被分配给重定向的响应（重定向后的最终URL）。

        - Response.request.url并不总是等于Response.url

        - 此属性仅在爬虫代码和
          :ref:`Spider Middlewares <topics-spider-middleware>` 中可用 ，但在下载器中间件中不可用（尽管您可以通过其他方式获得请求）和 :signal:`response_downloaded` 信号的处理程序。

    .. attribute:: Response.meta

        :attr:`Response.request` 对象（即 ``self.request.meta``) 的 `Request.meta` 属性的快捷方式。

        与:attr:`Response.request` 属性不同，:attr:`Response.meta` 
        属性沿着重定向和重试传播，因此您将获得 :attr:`Request.meta` 从爬虫发送的原始属性。

        .. seealso:: :attr:`Request.meta` 属性

    .. attribute:: Response.flags

        包含此响应标志的列表。标志是用于标记响应的标签。例如: `'cached'`, `'redirected`'等. 它们显示在响应 (`__str__`
        方法) 的字符串表示中，引擎用于记录。

    .. method:: Response.copy()

       返回一个新的Response，它是此Response的副本。

    .. method:: Response.replace([url, status, headers, body, request, flags, cls])

       返回具有相同成员的Response对象，但通过指定的任何关键字参数给定新值的成员除外。:attr:`Response.meta` 默认情况下复制该属性。

    .. method:: Response.urljoin(url)

        通过将Response :attr:`url` 与可能的相对URL 组合来构造绝对URL。

        这是 `urlparse.urljoin`_的包装器，它只是进行此调用的别名::

            urlparse.urljoin(response.url, url)

    .. automethod:: Response.follow


.. _urlparse.urljoin: https://docs.python.org/2/library/urlparse.html#urlparse.urljoin

.. _topics-request-response-ref-response-subclasses:

Response 子类
===================

以下是可用的内置Response子类列表。您还可以将Response类子类化以实现您自己的功能。

TextResponse 对象
--------------------

.. class:: TextResponse(url, [encoding[, ...]])

    :class:`TextResponse` 对象向基本
    :class:`Response` 类添加编码功能, 该基 类仅用于二进制数据，例如图像，声音或任何媒体文件。

    :class:`TextResponse` 除了基础 :class:`Response` 对象之外，对象还支持新的构造函数参数。其余功能与 :class:`Response` 类相同，此处未记录。

    :param encoding: i是一个字符串，其中包含用于此响应的编码。如果 :class:`TextResponse` 使用unicode主体创建对象，则将使用此编码对其进行编码（请记住body属性始终为字符串）。如果 ``encoding`` 是 ``None`` 默认值），则将在响应标头和正文中查找编码。
    :type encoding: string

    :class:`TextResponse` 除了标准属性之外，对象还支持以下属性 :class:`Response` ones:

    .. attribute:: TextResponse.text

       响应体，作为unicode。

       与之相同 ``response.body.decode(response.encoding)``, 但结果在第一次调用后缓存，因此您可以
       ``response.text``多次访问 而无需额外开销。

       .. note::

            ``unicode(response.body)`` 不是将响应体转换为unicode的正确方法：您将使用系统默认编码（通常为ascii）而不是响应编码。


    .. attribute:: TextResponse.encoding

       带有此响应编码的字符串。通过按顺序尝试以下机制来解决编码:

       1. 在构造函数编码参数中传递的编码 `encoding` argument

       2. 在Content-Type HTTP标头中声明的编码。如果此编码无效（即未知），则忽略该编码并尝试下一个解析机制。

       3. 响应正文中声明的编码。TextResponse类不为此提供任何特殊功能。但是，
          :class:`HtmlResponse` 和 :class:`XmlResponse` 类一样。

       4. 通过查看响应体来推断编码。这是一种更脆弱的方法，也是最后一种尝试的方法。

    .. attribute:: TextResponse.selector

        :class:`~scrapy.selector.Selector` 使用响应作为目标实例。选择器在第一次访问时被懒惰地实例化。

    :class:`TextResponse` 除了标准方法之外，对象还支持以下方法 :class:`Response` :

    .. method:: TextResponse.xpath(query)

        快捷方式 ``TextResponse.selector.xpath(query)``::

            response.xpath('//p')

    .. method:: TextResponse.css(query)

        A shortcut to ``TextResponse.selector.css(query)``::

            response.css('p')

    .. automethod:: TextResponse.follow

    .. method:: TextResponse.body_as_unicode()

        与 :attr:`text`,相同，但可用作方法。此方法是为了向后兼容而保留的；请首选 ``response.text``.


HtmlResponse 对象
--------------------

.. class:: HtmlResponse(url[, ...])

    The :class:`HtmlResponse` 类是 :class:`TextResponse`
   其增加了通过查看HTML编码自动发现支持 HTML `meta
    http-equiv`_ 属性。参见 :attr:`TextResponse.encoding`.

.. _meta http-equiv: https://www.w3schools.com/TAGS/att_meta_http_equiv.asp

XmlResponse 对象
-------------------

.. class:: XmlResponse(url[, ...])

    The :class:`XmlResponse` 类是 :class:`TextResponse` 的一个子类，它通过查看XML声明行来添加编码自动发现支持。参见 :attr:`TextResponse.encoding`.

.. _Twisted Failure: https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html
.. _lxml中的错误: https://bugs.launchpad.net/lxml/+bug/1665241
