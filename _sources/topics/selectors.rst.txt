.. _topics-selectors:

=========
选择器
=========

在抓取网页时，您需要执行的最常见任务是从HTML源提取数据。有几个库可用于实现此目的，例如：

 * `BeautifulSoup`_ 是Python程序员中非常流行的Web抓取库，它基于HTML代码的结构构造Python对象，并且还合理地处理坏标记，但它有一个缺点：它很慢。

 * `lxml`_ 是一个XML解析库（也可以解析HTML），它使用基于 `ElementTree`_的pythonic API 。（lxml不是Python标准库的一部分。）
 
Scrapy带有自己的提取数据机制。它们被称为选择器，因为它们“选择”由 `XPath`_ 或 `CSS`_ 表达式指定的HTML文档的某些部分。

`XPath`_ 是一种用于在XML文档中选择节点的语言，也可以与HTML一起使用。 
`CSS`_ 是一种将样式应用于HTML文档的语言。它定义选择器以将这些样式与特定HTML元素相关联。

.. note::
    Scrapy Selectors是一个围绕 `parsel`_ 库的包装器; 此包装器的目的是提供与Scrapy Response对象的更好集成。

    `parsel`_ 是一个独立的Web抓取库，可以在没有Scrapy的情况下使用。它使用了 `lxml`_ 库，并在lxml API之上实现了一个简单的API。这意味着Scrapy选择器的速度和解析精度与lxml非常相似。

.. _BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
.. _lxml: http://lxml.de/
.. _ElementTree: https://docs.python.org/2/library/xml.etree.elementtree.html
.. _cssselect: https://pypi.python.org/pypi/cssselect/
.. _XPath: https://www.w3.org/TR/xpath
.. _CSS: https://www.w3.org/TR/selectors
.. _parsel: https://parsel.readthedocs.io/

使用选择器
===============

构造选择器
----------------------

.. highlight:: python

Response 对象在 ``.selector`` 属性上演示:class:`~scrapy.selector.Selector`::

    >>> response.selector.xpath('//span/text()').get()
    'good'

Responses 使用 XPath 和 CSS 查询非常常见, 因此 Responses 包含两个快捷方式: ``response.xpath()`` 和 ``response.css()``::

    >>> response.xpath('//span/text()').get()
    'good'
    >>> response.css('span::text').get()
    'good'

Scrapy 选择器 :class:`~scrapy.selector.Selector` 
是通过 :class:`~scrapy.http.TextResponse` 对象或
标记作为unicode字符串（在 ``text`` 参数中）传递而构造的类的实例。
通常不需要手动构建Scrapy选择器:
``response`` 对象在Spider回调中可用，因此在大多数情况下使用 ``response.css()`` and ``response.xpath()``
快捷方式更方便。通过使用 ``response.selector`` 或者这些快捷方式之一，您还可以确保response主体只解析一次。

但是如果需要，可以直接使用 ``Selector`` .
从文本构造::

    >>> from scrapy.selector import Selector
    >>> body = '<html><body><span>good</span></body></html>'
    >>> Selector(text=body).xpath('//span/text()').get()
    'good'

从response构造 - :class:`~scrapy.http.HtmlResponse` 是
:class:`~scrapy.http.TextResponse` 的子类::

    >>> from scrapy.selector import Selector
    >>> from scrapy.http import HtmlResponse
    >>> response = HtmlResponse(url='http://example.com', body=body)
    >>> Selector(response=response).xpath('//span/text()').get()
    'good'

``Selector`` 根据输入类型自动选择最佳解析规则（XML与HTML）。

使用选择器
---------------

为了解释如何使用选择器，我们将使用Scrapy shell（提供交互式测试）和Scrapy文档服务器中的示例页面：

    https://docs.scrapy.org/en/latest/_static/selectors-sample1.html

.. _topics-selectors-htmlcode:

为了完整起见，这是完整的HTML代码：

.. literalinclude:: ../_static/selectors-sample1.html
   :language: html

.. highlight:: sh

首先，让我们打开shell::

    scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html

接着，当shell载入后，您将获得名为 ``response``
shell 变量, 并且在其  ``response.selector`` 属性上绑定了一个selector

由于我们正在处理HTML，因此选择器将自动使用HTML解析器。

.. highlight:: python

因此，通过查看该页面的 :ref:`HTML code <topics-selectors-htmlcode>` 让我们构建一个XPath来选择title标签内的文本::

    >>> response.xpath('//title/text()')
    [<Selector xpath='//title/text()' data='Example website'>]

实际上提取文本数据，必须调用选择器 ``.get()``
或 ``.getall()`` 方法，如下所示::

    >>> response.xpath('//title/text()').getall()
    ['Example website']
    >>> response.xpath('//title/text()').get()
    'Example website'

``.get()`` 总是返回一个结果; 如果有多个匹配，则返回第一个匹配的内容; 如果没有匹配项，则返回None。 ``.getall()`` 返回包含所有结果的列表。

请注意，CSS选择器可以使用CSS3伪元素选择文本或属性节点::

    >>> response.css('title::text').get()
    'Example website'

可以看到, ``.xpath()`` 和 ``.css()`` 方法返回一个
:class:`~scrapy.selector.SelectorList` 实例，它是一个新的选择器列表。此API可用于快速提取嵌套数据::

    >>> response.css('img').xpath('@src').getall()
    ['image1_thumb.jpg',
     'image2_thumb.jpg',
     'image3_thumb.jpg',
     'image4_thumb.jpg',
     'image5_thumb.jpg']

如果只想提取第一个匹配的元素，可以调用选择器 ``.get()`` (或 ``.extract_first()`` 在以前Scrapy版本中常用的别名）::

    >>> response.xpath('//div[@id="images"]/a/text()').get()
    'Name: My image 1 '

如果没有匹配的元素，则返回 None ``None`` ::

    >>> response.xpath('//div[@id="not-exists"]/text()').get() is None
    True

可以提供默认返回值作为参数，以代替 ``None``:

    >>> response.xpath('//div[@id="not-exists"]/text()').get(default='not-found')
    'not-found'

如果不使用 ``'@src'`` XPath 可以查询属性使用 :class:`~scrapy.selector.Selector` 的 ``.attrib`` 属性 ::

    >>> [img.attrib['src'] for img in response.css('img')]
    ['image1_thumb.jpg',
     'image2_thumb.jpg',
     'image3_thumb.jpg',
     'image4_thumb.jpg',
     'image5_thumb.jpg']

作为快捷方式, ``.attrib`` 也可以直接在SelectorList上使用; 它返回第一个匹配元素的属性::

    >>> response.css('img').attrib['src']
    'image1_thumb.jpg'

当只期望一个结果时，例如当选择按ID或选择网页上的唯一元素,这是最有用的::

    >>> response.css('base').attrib['href']
    'http://example.com/'

现在我们将获得基本URL和一些图像链接::

    >>> response.xpath('//base/@href').get()
    'http://example.com/'

    >>> response.css('base::attr(href)').get()
    'http://example.com/'

    >>> response.css('base').attrib['href']
    'http://example.com/'

    >>> response.xpath('//a[contains(@href, "image")]/@href').getall()
    ['image1.html',
     'image2.html',
     'image3.html',
     'image4.html',
     'image5.html']

    >>> response.css('a[href*=image]::attr(href)').getall()
    ['image1.html',
     'image2.html',
     'image3.html',
     'image4.html',
     'image5.html']

    >>> response.xpath('//a[contains(@href, "image")]/img/@src').getall()
    ['image1_thumb.jpg',
     'image2_thumb.jpg',
     'image3_thumb.jpg',
     'image4_thumb.jpg',
     'image5_thumb.jpg']

    >>> response.css('a[href*=image] img::attr(src)').getall()
    ['image1_thumb.jpg',
     'image2_thumb.jpg',
     'image3_thumb.jpg',
     'image4_thumb.jpg',
     'image5_thumb.jpg']

.. _topics-selectors-css-extensions:

CSS选择器扩展
---------------------------

根据W3C标准， `CSS selectors`_ 不支持选择文本节点或属性值。但是在Web抓取环境中选择这些是非常重要的，Scrapy（parsel）实现了一些 **非标准的伪元素**:

* 要选择文本节点，请使用 ``::text``
* 选择属性值，用 ``::attr(name)``  *name* 是你想要处理的值的属性的名称

.. warning::
    这些伪元素是Scrapy- / Parsel特有的。它们很可能不适用于其他库，如
    `lxml`_ 或 `PyQuery`_.

.. _PyQuery: https://pypi.python.org/pypi/pyquery

例:

* ``title::text``  选择 ``<title>`` 元素的文本节点::

    >>> response.css('title::text').get()
    'Example website'

* ``*::text``  选择当前选择器全文的所有文本节点::

    >>> response.css('#images *::text').getall()
    ['\n   ',
     'Name: My image 1 ',
     '\n   ',
     'Name: My image 2 ',
     '\n   ',
     'Name: My image 3 ',
     '\n   ',
     'Name: My image 4 ',
     '\n   ',
     'Name: My image 5 ',
     '\n  ']

* ``foo::text``  如果存在 ``foo`` 元素，但不包含文本（即文本为空），则不返回任何结果。::

    >>> response.css('img::text').getall()
    []

  ``.css('foo::text').get()``  这意味着元素存在并返回None. 如果始终需要字符串，请使用 ``default=''`` ::

    >>> response.css('img::text').get()
    >>> response.css('img::text').get(default='')
    ''

* ``a::attr(href)``  选择链接的 *href* 属性值::

    >>> response.css('a::attr(href)').getall()
    ['image1.html',
     'image2.html',
     'image3.html',
     'image4.html',
     'image5.html']

.. note::
    另请参阅: :ref:`selecting-attributes`.

.. note::
    你不能链接这些伪元素。但实际上它没有多大意义：文本节点没有属性，属性值已经是字符串值，并且没有子节点。

.. _CSS Selectors: https://www.w3.org/TR/css3-selectors/#selectors

.. _topics-selectors-nesting-selectors:

嵌套选择器
-----------------

选择器方法 (``.xpath()`` or ``.css()``) 返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子::

    >>> links = response.xpath('//a[contains(@href, "image")]')
    >>> links.getall()
    ['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg"></a>',
     '<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg"></a>',
     '<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg"></a>',
     '<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg"></a>',
     '<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg"></a>']

    >>> for index, link in enumerate(links):
    ...     args = (index, link.xpath('@href').get(), link.xpath('img/@src').get())
    ...     print('Link number %d points to url %r and image %r' % args)

    Link number 0 points to url 'image1.html' and image 'image1_thumb.jpg'
    Link number 1 points to url 'image2.html' and image 'image2_thumb.jpg'
    Link number 2 points to url 'image3.html' and image 'image3_thumb.jpg'
    Link number 3 points to url 'image4.html' and image 'image4_thumb.jpg'
    Link number 4 points to url 'image5.html' and image 'image5_thumb.jpg'

.. _selecting-attributes:

选择元素属性
----------------------------

有几种方法可以获取属性的值。首先，可以使用XPath语法::

    >>> response.xpath("//a/@href").getall()
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']

xpath语法有几个优点：它是一个标准的xpath特性，而 ``@attributes`` 可以用于xpath表达式的其他部分，例如，可以按属性值进行筛选。

Scrapy还提供了CSS选择器 (``::attr(...)``) 的扩展，它允许获取属性值::

    >>> response.css('a::attr(href)').getall()
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']

除此之外,还有 Selector ``.attrib`` 的属性. 如果您更喜欢在Python代码中查找属性而不使用XPath或CSS扩展，则可以使用它::

    >>> [a.attrib['href'] for a in response.css('a')]
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']

SelectorList也提供此属性; 它返回一个包含第一个匹配元素属性的字典。当期望选择器给出单个结果时（例如，当按元素ID选择时，或者在页面上选择唯一元素时），使用它很方便::

    >>> response.css('base').attrib
    {'href': 'http://example.com/'}
    >>> response.css('base').attrib['href']
    'http://example.com/'

空SelectorList的 ``.attrib`` 属性为空::

    >>> response.css('foo').attrib
    {}

使用具有正则表达式的选择器
----------------------------------------

:class:`~scrapy.selector.Selector` 还有一种``.re()`` 使用正则表达式提取数据的方法。 然而，不同于使用 ``.xpath()`` 或
``.css()`` 方法, ``.re()`` r方法返回unicode字符串的列表。所以你无法构造嵌套的 ``.re()`` 调用.

以下是用于从上面的 :ref:`HTML code
<topics-selectors-htmlcode>` 提取图像名称的示例::

    >>> response.xpath('//a[contains(@href, "image")]/text()').re(r'Name:\s*(.*)')
    ['My image 1',
     'My image 2',
     'My image 3',
     'My image 4',
     'My image 5']

还有一个额外的辅助函数 ``.get()`` （及其别名 ``.extract_first()``) 关于 ``.re()``, 命名为``.re_first()``.
使用它只提取第一个匹配的字符串::

    >>> response.xpath('//a[contains(@href, "image")]/text()').re_first(r'Name:\s*(.*)')
    'My image 1'

.. _old-extraction-api:

extract() 和 extract_first()
-----------------------------

如果你是一个长期的Scrapy用户，你可能熟悉 ``.extract()`` 和 ``.extract_first()`` 选择方法。许多博客文章和教程也在使用它们。Scrapy仍然支持这些方法，没有计划弃用它们。

但是，Scrapy文档现在使用 ``.get()`` 和
``.getall()`` 方法编写。我们认为这些新方法可以生成更简洁，更易读的代码。

以下示例显示了这些方法如何相互映射。

1. ``SelectorList.get()`` 与 ``SelectorList.extract_first()`` 相同::

     >>> response.css('a::attr(href)').get()
     'image1.html'
     >>> response.css('a::attr(href)').extract_first()
     'image1.html'

2. ``SelectorList.getall()`` 与 ``SelectorList.extract()`` 相同::

     >>> response.css('a::attr(href)').getall()
     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
     >>> response.css('a::attr(href)').extract()
     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']

2. ``Selector.get()`` 与 ``Selector.extract()`` 相同::

     >>> response.css('a::attr(href)')[0].get()
     'image1.html'
     >>> response.css('a::attr(href)')[0].extract()
     'image1.html'

4. 为了保持一致,还有 ``Selector.getall()``, 它返回一个列表::

    >>> response.css('a::attr(href)')[0].getall()
    ['image1.html']

因此，主要区别在于 ``.get()`` 和 ``.getall()`` 方法的输出更具可预测性: ``.get()`` 始终返回单个结果, ``.getall()``
始终返回所有提取结果的列表. 使用 ``.extract()`` 方法，结果是否是列表并不总是显而易见的; 要获得单个结果,可以调用 ``.extract()`` 或 ``.extract_first()`` 方法.


.. _topics-selectors-xpaths:
使用 XPaths
===================

这里有一些技巧可以帮助您有效地使用XPath和Scrapy选择器。如果您还不熟悉XPath，可能需要先看看这个 `XPath tutorial`_.

.. note::
    一些提示基于来自 `ScrapingHub`_ 博客的文章.

.. _`XPath tutorial`: http://www.zvon.org/comp/r/tut-XPath_1.html
.. _`ScrapingHub`: https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/


.. _topics-selectors-relative-xpaths:

相对XPath的使用
----------------------------

请记住，如果您嵌套选择器并使用起始为 ``/``, 的 XPath，该XPath将对文档使用绝对路径，而且对于你调用的 Selector 不是相对路径。

例如，假设您要提取 ``<p>`` 元素内的所有 ``<div>``
元素。首先，您将获得所有 ``<div>`` 元素::

    >>> divs = response.xpath('//div')

首先，你可能会使用下面的方法，这是错误的，因为它实际上从文档中提取所有 ``<p>`` 元素，而不仅仅是 ``<div>`` 元素内的元素::

    >>> for p in divs.xpath('//p'):  # this is wrong - gets all <p> from the whole document
    ...     print(p.get())

这是正确的方法（注意前缀为 ``.//p`` XPath 的点）::

    >>> for p in divs.xpath('.//p'):  # extracts all <p> inside
    ...     print(p.get())

另一个常见的情况是提取所有直接的子标签 ``<p>`` children::

    >>> for p in divs.xpath('p'):
    ...     print(p.get())

有关相对XPath的更多详细信息，请参阅XPath规范中的 `Location Paths`_ 部分.

.. _Location Paths: https://www.w3.org/TR/xpath#location-paths

按类查询时，请考虑使用CSS
------------------------------------------

因为一个元素可以包含多个CSS类，所以按类选择元素的XPath方式相当冗长::

    *[contains(concat(' ', normalize-space(@class), ' '), ' someclass ')]

如果你使用 ``@class='someclass'`` 可能会导致缺少具有其他类的元素，如果您只是使用 ``contains(@class, 'someclass')`` 来弥补这一点，那么您可能会得到更多想要的元素,如果它们具有共享字符串 ``someclass`` 的不同类名.

事实证明，Scrapy选择器允许您链接选择器，因此大多数情况下您可以使用CSS按类选择，然后在需要时切换到XPath::

    >>> from scrapy import Selector
    >>> sel = Selector(text='<div class="hero shout"><time datetime="2014-07-23 19:00">Special date</time></div>')
    >>> sel.css('.shout').xpath('./time/@datetime').getall()
    ['2014-07-23 19:00']

这比使用上面显示的详细XPath技巧更清晰。只需记住 ``.`` 在后面的XPath表达式中使用它。

注意 //node[1] 和 (//node)[1] 之间的区别
----------------------------------------------------------

``//node[1]`` 选择在其各自父节点下首先出现的所有节点.

``(//node)[1]`` 选择文档中的所有节点，然后只获取其中的第一个节点.

例::

    >>> from scrapy import Selector
    >>> sel = Selector(text="""
    ....:     <ul class="list">
    ....:         <li>1</li>
    ....:         <li>2</li>
    ....:         <li>3</li>
    ....:     </ul>
    ....:     <ul class="list">
    ....:         <li>4</li>
    ....:         <li>5</li>
    ....:         <li>6</li>
    ....:     </ul>""")
    >>> xp = lambda x: sel.xpath(x).getall()

这将获取其父级下的所有第一个 ``<li>`` 元素::

    >>> xp("//li[1]")
    ['<li>1</li>', '<li>4</li>']

这将得到整个文档中的第一个 ``<li>`` 元素::

    >>> xp("(//li)[1]")
    ['<li>1</li>']

这将获取 ``<ul>`` 元素下第一个 ``<li>`` 元素::

    >>> xp("//ul/li[1]")
    ['<li>1</li>', '<li>4</li>']

这将得到整个文档中 ``<ul>`` 元素的第一个 ``<li>`` 元素::

    >>> xp("(//ul/li)[1]")
    ['<li>1</li>']

在条件中使用文本节点
-------------------------------

当您需要使用文本内容作为 `XPath string function`_,
参数时，请避免使用 ``.//text()`` 而使用 ``.`` 代替.

这是因为表达式 ``.//text()`` 产生一组文本元素 -- 一个 *node-set*.
一个 node-set 被转换为一个字符串，当它作为参数传递给一个字符串函数，如 ``contains()`` 或 ``starts-with()`` 时，会产生第一个元素的文本。

例::

    >>> from scrapy import Selector
    >>> sel = Selector(text='<a href="#">Click here to go to the <strong>Next Page</strong></a>')

将 node-set 转换为字符串::

    >>> sel.xpath('//a//text()').getall() # take a peek at the node-set
    ['Click here to go to the ', 'Next Page']
    >>> sel.xpath("string(//a[1]//text())").getall() # convert it to string
    ['Click here to go to the ']

然而，转换为字符串的 *节点* 会将自身的文本加上其所有后代::

    >>> sel.xpath("//a[1]").getall() # select the first node
    ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']
    >>> sel.xpath("string(//a[1])").getall() # convert it to string
    ['Click here to go to the Next Page']

因此，在这种情况下，使用 ``.//text()``  node-set 不会选择任何东西::

    >>> sel.xpath("//a[contains(.//text(), 'Next Page')]").getall()
    []
    
但使用 ``.`` 表示节点有效::

    >>> sel.xpath("//a[contains(., 'Next Page')]").getall()
    ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']

.. _`XPath string function`: https://www.w3.org/TR/xpath/#section-String-Functions

.. _topics-selectors-xpath-variables:

XPath表达式中的变量
------------------------------

XPath允许您使用 ``$somevariable`` 语法引用XPath表达式中的变量。这有点类似于SQL世界中的参数化查询或预处理语句，您可以使用占位符替换查询中的某些参数 ``?``,
然后将其替换为随查询传递的值。

下面是一个根据元素的“id”属性值匹配元素的示例，不需要对其进写死(可变表达式)（如前所示）：::

    >>> # `$val` used in the expression, a `val` argument needs to be passed
    >>> response.xpath('//div[@id=$val]/a/text()', val='images').get()
    'Name: My image 1 '

这是另一个例子，找到 ``<div>`` 包含五个 ``<a>`` 子节点的标签的“id”属性（这里我们将值 ``5`` 作为整数传递）::

    >>> response.xpath('//div[count(a)=$cnt]/@id', cnt=5).get()
    'images'

调用 ``.xpath()`` 时，所有变量引用都必须有一个绑定值
(否则您将获得 ``ValueError: XPath error:`` 异常).
这是通过根据需要传递尽可能多的命名参数来完成的。

`parsel`_, 为Scrapy选择器提供动力的库, 有关于 `XPath variables`_ 更多细节和示例。.

.. _XPath variables: https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions


.. _removing-namespaces:

删除命名空间
-------------------

当处理爬虫项目时，移除命名空间而仅仅使用元素名称通常是很方便的，编写更简单/方便的XPath。您可以使用
:meth:`Selector.remove_namespaces` 方法.

Let's show an example that illustrates this with the Python Insider blog atom feed.
让我们来看一个例子，用Python Insider博客来说明这一点。
.. highlight:: sh

首先，我们打开带有我们要抓取的url的shell::

    $ scrapy shell https://feeds.feedburner.com/PythonInsider

这是文件的开始::

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet ...
    <feed xmlns="http://www.w3.org/2005/Atom"
          xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/"
          xmlns:blogger="http://schemas.google.com/blogger/2008"
          xmlns:georss="http://www.georss.org/georss"
          xmlns:gd="http://schemas.google.com/g/2005"
          xmlns:thr="http://purl.org/syndication/thread/1.0"
          xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">
      ...

你可以看到几个命名空间声明包含一个默认的
"http://www.w3.org/2005/Atom" 并使用另一个"GD":”前缀“
"http://schemas.google.com/g/2005".

.. highlight:: python

一旦进入shell，我们可以尝试选择所有的 ``<link>`` 对象，可以看到没有结果(因为Atom XML命名空间混淆了这些节点)::

    >>> response.xpath("//link")
    []

但是一旦我们调用 :meth:`Selector.remove_namespaces` 方法，所有节点都可以直接通过它们的名字访问::

    >>> response.selector.remove_namespaces()
    >>> response.xpath("//link")
    [<Selector xpath='//link' data='<link rel="alternate" type="text/html" h'>,
     <Selector xpath='//link' data='<link rel="next" type="application/atom+'>,
     ...

如果你对为什么命名空间移除操作并不总是被调用，而需要手动调用有疑惑。这是因为存在如下两个原因，按照相关顺序如下:

1. 移除命名空间需要迭代并修改文件的所有节点，而这对于Scrapy爬取的所有文档操作需要一定的性能消耗。

2. 会存在这样的情况，确实需要使用命名空间，但有些元素的名字与命名空间冲突。尽管这些情况非常少见。


使用EXSLT扩展
----------------------

因构建在 `lxml`_之上，Scrapy选择器也支持一些 `EXSLT`_ 扩展，可以在XPath表达式中使用这些预先制定的命名空间:


======  =====================================    =======================
前缀    命名空间                                  用法
======  =====================================    =======================
re      \http://exslt.org/regular-expressions    `regular expressions`_
set     \http://exslt.org/sets                   `set manipulation`_
======  =====================================    =======================

正则表达式
~~~~~~~~~~~~~~~~~~~

例如，当XPath的 ``starts-with()`` 或 ``contains()`` 不能满足需求时， ``test()`` 函数可以证明是非常有用的。

例如在列表中选择有”class”元素且结尾为一个数字的链接::

    >>> from scrapy import Selector
    >>> doc = u"""
    ... <div>
    ...     <ul>
    ...         <li class="item-0"><a href="link1.html">first item</a></li>
    ...         <li class="item-1"><a href="link2.html">second item</a></li>
    ...         <li class="item-inactive"><a href="link3.html">third item</a></li>
    ...         <li class="item-1"><a href="link4.html">fourth item</a></li>
    ...         <li class="item-0"><a href="link5.html">fifth item</a></li>
    ...     </ul>
    ... </div>
    ... """
    >>> sel = Selector(text=doc, type="html")
    >>> sel.xpath('//li//@href').getall()
    ['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']
    >>> sel.xpath('//li[re:test(@class, "item-\d$")]//@href').getall()
    ['link1.html', 'link2.html', 'link4.html', 'link5.html']
    >>>

.. warning:: C语言库 ``libxslt`` 本身不支持EXSLT正则表达式，所以lxml在实现时使用Python的 `lxml`_'s implementation uses hooks to Python's ``re`` 模块钩子。因此，在XPath表达式中使用regexp函数可能会增加小的性能损失。

集合操作
~~~~~~~~~~~~~~

集合操作可以方便地在提取文本元素之前去除文档树中的一部分内容。

使用itemscopes组和相应的itemprops组提取微数据（从 http://schema.org/Product 获取的示例内容)
示例::

    >>> doc = u"""
    ... <div itemscope itemtype="http://schema.org/Product">
    ...   <span itemprop="name">Kenmore White 17" Microwave</span>
    ...   <img src="kenmore-microwave-17in.jpg" alt='Kenmore 17" Microwave' />
    ...   <div itemprop="aggregateRating"
    ...     itemscope itemtype="http://schema.org/AggregateRating">
    ...    Rated <span itemprop="ratingValue">3.5</span>/5
    ...    based on <span itemprop="reviewCount">11</span> customer reviews
    ...   </div>
    ...
    ...   <div itemprop="offers" itemscope itemtype="http://schema.org/Offer">
    ...     <span itemprop="price">$55.00</span>
    ...     <link itemprop="availability" href="http://schema.org/InStock" />In stock
    ...   </div>
    ...
    ...   Product description:
    ...   <span itemprop="description">0.7 cubic feet countertop microwave.
    ...   Has six preset cooking categories and convenience features like
    ...   Add-A-Minute and Child Lock.</span>
    ...
    ...   Customer reviews:
    ...
    ...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">
    ...     <span itemprop="name">Not a happy camper</span> -
    ...     by <span itemprop="author">Ellie</span>,
    ...     <meta itemprop="datePublished" content="2011-04-01">April 1, 2011
    ...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">
    ...       <meta itemprop="worstRating" content = "1">
    ...       <span itemprop="ratingValue">1</span>/
    ...       <span itemprop="bestRating">5</span>stars
    ...     </div>
    ...     <span itemprop="description">The lamp burned out and now I have to replace
    ...     it. </span>
    ...   </div>
    ...
    ...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">
    ...     <span itemprop="name">Value purchase</span> -
    ...     by <span itemprop="author">Lucas</span>,
    ...     <meta itemprop="datePublished" content="2011-03-25">March 25, 2011
    ...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">
    ...       <meta itemprop="worstRating" content = "1"/>
    ...       <span itemprop="ratingValue">4</span>/
    ...       <span itemprop="bestRating">5</span>stars
    ...     </div>
    ...     <span itemprop="description">Great microwave for the price. It is small and
    ...     fits in my apartment.</span>
    ...   </div>
    ...   ...
    ... </div>
    ... """
    >>> sel = Selector(text=doc, type="html")
    >>> for scope in sel.xpath('//div[@itemscope]'):
    ...     print("current scope:", scope.xpath('@itemtype').getall())
    ...     props = scope.xpath('''
    ...                 set:difference(./descendant::*/@itemprop,
    ...                                .//*[@itemscope]/*/@itemprop)''')
    ...     print("    properties: %s" % (props.getall()))
    ...     print("")

    current scope: ['http://schema.org/Product']
        properties: ['name', 'aggregateRating', 'offers', 'description', 'review', 'review']

    current scope: ['http://schema.org/AggregateRating']
        properties: ['ratingValue', 'reviewCount']

    current scope: ['http://schema.org/Offer']
        properties: ['price', 'availability']

    current scope: ['http://schema.org/Review']
        properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']

    current scope: ['http://schema.org/Rating']
        properties: ['worstRating', 'ratingValue', 'bestRating']

    current scope: ['http://schema.org/Review']
        properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']

    current scope: ['http://schema.org/Rating']
        properties: ['worstRating', 'ratingValue', 'bestRating']

    >>>

在这里，我们首先在 ``itemscope`` 元素上迭代，对于其中的每一个元素，我们寻找所有的 ``itemprops`` 元素，并排除那些本身在另一个 ``itemscope`` 内的元素。

.. _EXSLT: http://exslt.org/
.. _regular expressions: http://exslt.org/regexp/index.html
.. _set manipulation: http://exslt.org/set/index.html

其他XPath扩展
----------------------

Scrapy选择器还提供了一个非常不错的XPath扩展函数
``has-class`` 该函数 返回 ``True`` 具有所有指定HTML类的节点。

.. highlight:: html

对于以下HTML::

    <p class="foo bar-baz">First</p>
    <p class="foo">Second</p>
    <p class="bar">Third</p>
    <p>Fourth</p>

.. highlight:: python

你可以像这样使用它::

    >>> response.xpath('//p[has-class("foo")]')
    [<Selector xpath='//p[has-class("foo")]' data='<p class="foo bar-baz">First</p>'>,
     <Selector xpath='//p[has-class("foo")]' data='<p class="foo">Second</p>'>]
    >>> response.xpath('//p[has-class("foo", "bar-baz")]')
    [<Selector xpath='//p[has-class("foo", "bar-baz")]' data='<p class="foo bar-baz">First</p>'>]
    >>> response.xpath('//p[has-class("foo", "bar")]')
    []

所以 XPath ``//p[has-class("foo", "bar-baz")]`` 大致相当于 CSS
``p.foo.bar-baz``.  请注意，在大多数情况下，它的速度较慢，因为它是一个纯Python函数，可以为问题中的每个节点调用，而CSS查找被转换为xpath，因此运行效率更高，因此性能方面，它的使用仅限于不容易用css选择器描述的情况。

Parsel还简化了添加自己的XPath扩展。

.. autofunction:: parsel.xpathfuncs.set_xpathfunc


.. _topics-selectors-ref:

内置选择器参考
============================

.. module:: scrapy.selector
   :synopsis: Selector class

Selector 对象
----------------

.. autoclass:: Selector

  .. automethod:: xpath

      .. note::

          为方便起见，可以将此方法称为 ``response.xpath()``

  .. automethod:: css

      .. note::

          为方便起见，可以将此方法称为 ``response.css()``

  .. automethod:: get

     另请参阅: :ref:`old-extraction-api`

  .. autoattribute:: attrib

     另请参阅: :ref:`selecting-attributes`.

  .. automethod:: re

  .. automethod:: re_first

  .. automethod:: register_namespace

  .. automethod:: remove_namespaces

  .. automethod:: __bool__

  .. automethod:: getall

     将此方法添加到Selector以保持一致性; 它对SelectorList更有用。另请参见: :ref:`old-extraction-api`

SelectorList 对象
--------------------

.. autoclass:: SelectorList

   .. automethod:: xpath

   .. automethod:: css

   .. automethod:: getall

      另请参见: :ref:`old-extraction-api`

   .. automethod:: get

      另请参见: :ref:`old-extraction-api`

   .. automethod:: re

   .. automethod:: re_first

   .. autoattribute:: attrib

      另请参见: :ref:`selecting-attributes`.

.. _selector-examples:

示例
========

.. _selector-examples-html:

HTML响应的选择器示例
----------------------------------

这里有几个 :class:`Selector` 示例来说明几个概念。在所有情况下，我们假设已经有一个通过 :class:`~scrapy.http.HtmlResponse`   对象实例化的 :class:`Selector` ,像这样::

      sel = Selector(html_response)

1. 从HTML响应正文中选择所有 ``<h1>`` 元素，返回
   :class:`Selector` 对象的列表（即 :class:`SelectorList` 对象)::

      sel.xpath("//h1")

2. 从HTML响应正文中提取所有 ``<h1>`` 元素的文本，返回unicode字符串列表::

      sel.xpath("//h1").getall()         # this includes the h1 tag
      sel.xpath("//h1/text()").getall()  # this excludes the h1 tag

3. 在所有 ``<p>`` 标签上迭代，打印它们的类属性::

      for node in sel.xpath("//p"):
          print(node.attrib['class'])


.. _selector-examples-xml:

XML响应的选择器示例
---------------------------------

这里有几个例子来说明几个概念。在这两种情况下，我们假设已经有一个通过 :class:`~scrapy.http.XmlResponse` 对象实例化的 :class:`Selector` 如下所示::

      sel = Selector(xml_response)

1. 从HTML响应正文中选择所有 ``<product>``  元素，返回 :class:`Selector` 对象的列表（即 :class:`SelectorList` 对象)::

      sel.xpath("//product")

2. 从 `Google Base XML feed`_ 中提取所有的有价值的数据，这需要注册一个命名空间::

      sel.register_namespace("g", "http://base.google.com/ns/1.0")
      sel.xpath("//g:price").getall()

.. _Google Base XML feed: https://support.google.com/merchants/answer/160589?hl=en&ref_topic=2473799
