.. _topics-shell:

============
Scrapy shell
============

Scrapy shell是一个交互式shell，您可以在其中快速尝试和调试您的抓取代码，而无需运行spider程序。它意味着用于测试数据提取代码，但你实际上可以使用它来测试任何类型的代码，因为它也是一个普通的Python shell。

Tshell用于测试XPath或CSS表达式，并查看它们如何工作，以及他们从您要尝试抓取的网页中提取的数据。它允许您在编写spider时交互式测试表达式，而无需运行spider来测试每个更改。

一旦您熟悉了Scrapy shell，你会发现它是开发和调试你的spider的一个非常宝贵的工具。

配置
=====================

如果您安装了 `IPython`_ ，Scrapy shell 会使用它（而不是标准的Python控制台）。 `IPython`_ 控制台更强大，并提供智能自动补全和高亮输出，等等。

我们强烈推荐您安装 `IPython`_, 特别是如果您使用Unix系统 ( `IPython`_ 在Unix下工作的很好). 详情请参考 `IPython installation guide`_
for more info.

Scrapy还支持 `bpython`_, 并且将尝试在 `IPython`_
不可用时使用它。

通过scrapy的设置，您可以将其配置为使用
``ipython``, ``bpython`` 或标准 ``python`` shell中的任何一个，而不管它们是否安装。这是通过设置 ``SCRAPY_PYTHON_SHELL`` 环境变量来完成的;或者通过在 :ref:`scrapy.cfg <topics-config-settings>` 中定义它::

    [settings]
    shell = bpython

.. _IPython: https://ipython.org/
.. _IPython installation guide: https://ipython.org/install.html
.. _bpython: https://www.bpython-interpreter.org/

启动
================

您可以使用 :command:`shell` 来启动Scrapy终端::

    scrapy shell <url>

 ``<url>`` 是您要爬取的网页的地址。

:command:`shell` 也适用于本地文件。如果你想玩一个本地的网页副本，这可以很方便。 :command:`shell` 抓取本地文件的语法如下::

    # UNIX-style
    scrapy shell ./path/to/file.html
    scrapy shell ../other/path/to/file.html
    scrapy shell /absolute/path/to/file.html

    # File URI
    scrapy shell file:///absolute/path/to/file.html

.. note:: 注意！ 当使用相对文件路径时，应显式地使用 ``./`` (或 ``../`` 如果相关)前缀.
    ``scrapy shell index.html``  不会正常工作（这是设计，而不是一个错误）。

    因为 :command:`shell` 优先于文件URI的HTTP URL,
    并且 ``index.html`` 在语法上类似于 ``example.com``,
    所以:command:`shell` 将 ``index.html`` 视为域名并触发DNS查找错误r::

        $ scrapy shell index.html
        [ ... scrapy shell starts ... ]
        [ ... traceback ... ]
        twisted.internet.error.DNSLookupError: DNS lookup failed:
        address 'index.html' not found: [Errno -5] No address associated with hostname.

    再次明确一点，如果在当前目录中存在一个名为 ``index.html`` 的文件,:command:`shell` w 将不会预先测试。

使用
===============

Scrapy shell只是一个普通的Python控制台 (或 `IPython`_ 控制台，如果你有它) 为方便起见，它提供了一些额外的快捷方式功能。

可用的快捷方式
-------------------

 * ``shelp()`` - 打印可用对象及快捷命令的帮助列表

 * ``fetch(url[, redirect=True])`` - 根据给定的request获取新的response并相应地更新所有相关对象。 您可以选择请求HTTP 3xx重定向不要在后面传递 ``redirect=False``

 * ``fetch(request)`` - 根据给定的request获取新的response并相应地更新所有相关对象。

 * ``view(response)`` - 在本机的浏览器打开给定的 response。 其会在response的body中添加一个 `\<base\>`_ 标签,以便正确显示外部链接（如图像和样式表）。但请注意，这将在您的计算机中创建一个临时文件，该文件不会自动删除。

.. _<base>: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base

可用的Scrapy对象
------------------------

Scrapy终端根据下载的页面会自动创建一些方便使用的对象，例如 :class:`~scrapy.http.Response` 对象及
:class:`~scrapy.selector.Selector` 对象(对HTML及XML内容)。

这些对象有:

 * ``crawler`` - 当前 :class:`~scrapy.crawler.Crawler` 对象.

 * ``spider`` - 处理URL的spider。 对当前URL没有处理的Spider时则为一个
   :class:`~scrapy.spiders.Spider` 对象.

 * ``request`` - 最近获取到的页面的 :class:`~scrapy.http.Request` 对象。 您可以使用 :meth:`~scrapy.http.Request.replace`
   修改该request。或者 使用  ``fetch``
   快捷方式来获取新的request。

 * ``response`` - 包含最近获取到的页面的 :class:`~scrapy.http.Response` 对象.

 * ``settings`` - 当前的 :ref:`Scrapy 设置 <topics-settings>`

shell会话示例
========================

下面是一个典型的shell会话示例，我们首先抓取 https://scrapy.org 页面，然后继续抓取 https://reddit.com 页面。最后，我们修改（Reddit）请求方法为POST并重新获取它得到一个错误。我们通过在Windows中键入Ctrl-D（在Unix系统中）或Ctrl-Z结束会话。

需要注意的是，由于爬取的页面不是静态页，内容会随着时间而修改， 因此例子中提取到的数据可能与您尝试的结果不同。 该例子的唯一目的是让您熟悉Scrapy shell。

首先，我们启动shell::

    scrapy shell 'https://scrapy.org' --nolog

然后，shell获取URL（使用Scrapy下载器）并打印可用对象和有用的快捷方式列表（您会注意到这些行都以 ``[s]`` 前缀开头)::

    [s] Available Scrapy objects:
    [s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
    [s]   crawler    <scrapy.crawler.Crawler object at 0x7f07395dd690>
    [s]   item       {}
    [s]   request    <GET https://scrapy.org>
    [s]   response   <200 https://scrapy.org/>
    [s]   settings   <scrapy.settings.Settings object at 0x7f07395dd710>
    [s]   spider     <DefaultSpider 'default' at 0x7f0735891690>
    [s] Useful shortcuts:
    [s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
    [s]   fetch(req)                  Fetch a scrapy.Request and update local objects
    [s]   shelp()           Shell help (print this help)
    [s]   view(response)    View response in a browser

    >>>


之后，我们可以开始使用对象::

    >>> response.xpath('//title/text()').get()
    'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'

    >>> fetch("https://reddit.com")

    >>> response.xpath('//title/text()').get()
    'reddit: the front page of the internet'

    >>> request = request.replace(method="POST")

    >>> fetch(request)

    >>> response.status
    404

    >>> from pprint import pprint

    >>> pprint(response.headers)
    {'Accept-Ranges': ['bytes'],
     'Cache-Control': ['max-age=0, must-revalidate'],
     'Content-Type': ['text/html; charset=UTF-8'],
     'Date': ['Thu, 08 Dec 2016 16:21:19 GMT'],
     'Server': ['snooserv'],
     'Set-Cookie': ['loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                    'loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                    'loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                    'loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure'],
     'Vary': ['accept-encoding'],
     'Via': ['1.1 varnish'],
     'X-Cache': ['MISS'],
     'X-Cache-Hits': ['0'],
     'X-Content-Type-Options': ['nosniff'],
     'X-Frame-Options': ['SAMEORIGIN'],
     'X-Moose': ['majestic'],
     'X-Served-By': ['cache-cdg8730-CDG'],
     'X-Timer': ['S1481214079.394283,VS0,VE159'],
     'X-Ua-Compatible': ['IE=edge'],
     'X-Xss-Protection': ['1; mode=block']}
    >>>


.. _topics-shell-inspect-response:

在spider中调用shell来查看response
====================================================

有时您想在spider的某个位置中查看被处理的response， 以确认您期望的response到达特定位置。

这可以通过 ``scrapy.shell.inspect_response`` 函数来实现。

以下是如何在spider中调用该函数的例子::

    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"
        start_urls = [
            "http://example.com",
            "http://example.org",
            "http://example.net",
        ]

        def parse(self, response):
            # We want to inspect one specific response.
            if ".org" in response.url:
                from scrapy.shell import inspect_response
                inspect_response(response, self)

            # Rest of parsing code.

当运行spider时，您将得到类似下列的输出::

    2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)
    2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.org> (referer: None)
    [s] Available Scrapy objects:
    [s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>
    ...

    >>> response.url
    'http://example.org'

然后，您可以检查提取代码是否正常工作::

    >>> response.xpath('//h1[@class="fn"]')
    []

您可以在Web浏览器中打开response，看看它是否是您期望的response::

    >>> view(response)
    True

最后您可以点击Ctrl-D(Windows下Ctrl-Z)来退出终端，恢复爬取::

    >>> ^D
    2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.net> (referer: None)
    ...

请注意，您不能在这里使用 ``fetch`` 快捷命令，因为Scrapy引擎被shell阻止。然而，在你离开shell之后，spider会继续爬到它停止的地方，如上图所示
