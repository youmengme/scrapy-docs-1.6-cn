

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Core API &mdash; Scrapy 1.6.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="信号(Signals)" href="signals.html" />
    <link rel="prev" title="扩展" href="extensions.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Scrapy
          

          
          </a>

          
            
            
              <div class="version">
                1.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">First steps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Scrapy 一目了然</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/install.html">安装指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/tutorial.html">Scrapy教程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/examples.html">示例</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="commands.html">命令行工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="spiders.html">爬虫（Spiders）</a></li>
<li class="toctree-l1"><a class="reference internal" href="selectors.html">选择器</a></li>
<li class="toctree-l1"><a class="reference internal" href="items.html">Items</a></li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Item 装载器</a></li>
<li class="toctree-l1"><a class="reference internal" href="shell.html">Scrapy shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="item-pipeline.html">Item Pipeline（项目管道）</a></li>
<li class="toctree-l1"><a class="reference internal" href="feed-exports.html">Feed 导出</a></li>
<li class="toctree-l1"><a class="reference internal" href="request-response.html">请求与响应</a></li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">链接提取器</a></li>
<li class="toctree-l1"><a class="reference internal" href="settings.html">设置</a></li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">例外</a></li>
</ul>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">统计收集</a></li>
<li class="toctree-l1"><a class="reference internal" href="email.html">发送电子邮件</a></li>
<li class="toctree-l1"><a class="reference internal" href="telnetconsole.html">Telnet 控制台</a></li>
<li class="toctree-l1"><a class="reference internal" href="webservice.html">Web 服务</a></li>
</ul>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="contracts.html">Spiders Contracts</a></li>
<li class="toctree-l1"><a class="reference internal" href="practices.html">Common Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="broad-crawls.html">Broad Crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="developer-tools.html">Using your browser’s Developer Tools for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="leaks.html">Debugging memory leaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="media-pipeline.html">下载和处理文件与图像</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">Deploying Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="autothrottle.html">AutoThrottle 扩展</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">Jobs: pausing and resuming crawls</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">架构概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="downloader-middleware.html">Downloader Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="spider-middleware.html">Spider Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">扩展</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Core API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#crawler-api">Crawler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-scrapy.settings">Settings API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-scrapy.loader">SpiderLoader API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-scrapy.signalmanager">Signals API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stats-collector-api">Stats Collector API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="signals.html">信号(Signals)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exporters.html">Item Exporters</a></li>
</ul>
<p class="caption"><span class="caption-text">All the rest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Scrapy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versioning.html">Versioning and API Stability</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Scrapy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Core API</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/topics/api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="core-api">
<span id="topics-api"></span><h1>Core API<a class="headerlink" href="#core-api" title="Permalink to this headline">¶</a></h1>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<p>This section documents the Scrapy core API, and it’s intended for developers of
extensions and middlewares.</p>
<div class="section" id="crawler-api">
<span id="topics-api-crawler"></span><h2>Crawler API<a class="headerlink" href="#crawler-api" title="Permalink to this headline">¶</a></h2>
<p>The main entry point to Scrapy API is the <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object, passed to extensions through the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method. This
object provides access to all Scrapy core components, and it’s the only way for
extensions to access them and hook their functionality into Scrapy.</p>
<span class="target" id="module-scrapy.crawler"></span><p>The Extension Manager is responsible for loading and keeping track of installed
extensions and it’s configured through the <a class="reference internal" href="settings.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting which
contains a dictionary of all available extensions and their order similar to
how you <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware-setting"><span class="std std-ref">configure the downloader middlewares</span></a>.</p>
<dl class="class">
<dt id="scrapy.crawler.Crawler">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.crawler.</code><code class="sig-name descname">Crawler</code><span class="sig-paren">(</span><em class="sig-param">spidercls</em>, <em class="sig-param">settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>The Crawler object must be instantiated with a
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spiders.Spider</span></code></a> subclass and a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.settings.Settings</span></code></a> object.</p>
<dl class="attribute">
<dt id="scrapy.crawler.Crawler.settings">
<code class="sig-name descname">settings</code><a class="headerlink" href="#scrapy.crawler.Crawler.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>The settings manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to access the Scrapy settings
of this crawler.</p>
<p>For an introduction on Scrapy settings see <a class="reference internal" href="settings.html#topics-settings"><span class="std std-ref">设置</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.signals">
<code class="sig-name descname">signals</code><a class="headerlink" href="#scrapy.crawler.Crawler.signals" title="Permalink to this definition">¶</a></dt>
<dd><p>The signals manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy
functionality.</p>
<p>For an introduction on signals see <a class="reference internal" href="signals.html#topics-signals"><span class="std std-ref">信号(Signals)</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalManager</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.stats">
<code class="sig-name descname">stats</code><a class="headerlink" href="#scrapy.crawler.Crawler.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>The stats collector of this crawler.</p>
<p>This is used from extensions &amp; middlewares to record stats of their
behaviour, or access stats collected by other extensions.</p>
<p>For an introduction on stats collection see <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">统计收集</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.extensions">
<code class="sig-name descname">extensions</code><a class="headerlink" href="#scrapy.crawler.Crawler.extensions" title="Permalink to this definition">¶</a></dt>
<dd><p>The extension manager that keeps track of enabled extensions.</p>
<p>Most extensions won’t need to access this attribute.</p>
<p>For an introduction on extensions and a list of available extensions on
Scrapy see <a class="reference internal" href="extensions.html#topics-extensions"><span class="std std-ref">扩展</span></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.engine">
<code class="sig-name descname">engine</code><a class="headerlink" href="#scrapy.crawler.Crawler.engine" title="Permalink to this definition">¶</a></dt>
<dd><p>The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.</p>
<p>Some extension may want to access the Scrapy engine, to inspect  or
modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.spider">
<code class="sig-name descname">spider</code><a class="headerlink" href="#scrapy.crawler.Crawler.spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Spider currently being crawled. This is an instance of the spider class
provided while constructing the crawler, and it is created after the
arguments given in the <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.crawl">
<code class="sig-name descname">crawl</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Starts the crawler by instantiating its spider class with the given
<cite>args</cite> and <cite>kwargs</cite> arguments, while setting the execution engine in
motion.</p>
<p>Returns a deferred that is fired when the crawl is finished.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.crawler.CrawlerRunner">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.crawler.</code><code class="sig-name descname">CrawlerRunner</code><span class="sig-paren">(</span><em class="sig-param">settings=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a convenient helper class that keeps track of, manages and runs
crawlers inside an already setup Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>.</p>
<p>The CrawlerRunner object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="practices.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.crawl">
<code class="sig-name descname">crawl</code><span class="sig-paren">(</span><em class="sig-param">crawler_or_spidercls</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <cite>crawler_or_spidercls</cite> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass or string) – already created crawler, or a spider class
or spider’s name inside the project to create it</p></li>
<li><p><strong>args</strong> (<em>list</em>) – arguments to initialize the spider</p></li>
<li><p><strong>kwargs</strong> (<em>dict</em>) – keyword arguments to initialize the spider</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.crawlers">
<em class="property">property </em><code class="sig-name descname">crawlers</code><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.create_crawler">
<code class="sig-name descname">create_crawler</code><span class="sig-paren">(</span><em class="sig-param">crawler_or_spidercls</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li><p>If <cite>crawler_or_spidercls</cite> is a Crawler, it is returned as-is.</p></li>
<li><p>If <cite>crawler_or_spidercls</cite> is a Spider subclass, a new Crawler
is constructed for it.</p></li>
<li><p>If <cite>crawler_or_spidercls</cite> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</p></li>
</ul>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.join">
<code class="sig-name descname">join</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawlers" title="scrapy.crawler.CrawlerRunner.crawlers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.stop">
<code class="sig-name descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.crawler.CrawlerProcess">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.crawler.</code><code class="sig-name descname">CrawlerProcess</code><span class="sig-paren">(</span><em class="sig-param">settings=None</em>, <em class="sig-param">install_root_handler=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a></p>
<p>A class to run multiple scrapy crawlers in a process simultaneously.</p>
<p>This class extends <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> by adding support
for starting a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> and handling shutdown signals, like the
keyboard interrupt command Ctrl-C. It also configures top-level logging.</p>
<p>This utility should be a better fit than
<a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> if you aren’t running another
Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> within your application.</p>
<p>The CrawlerProcess object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>install_root_handler</strong> – whether to install root logging handler
(default: True)</p>
</dd>
</dl>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="practices.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.crawl">
<code class="sig-name descname">crawl</code><span class="sig-paren">(</span><em class="sig-param">crawler_or_spidercls</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <cite>crawler_or_spidercls</cite> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass or string) – already created crawler, or a spider class
or spider’s name inside the project to create it</p></li>
<li><p><strong>args</strong> (<em>list</em>) – arguments to initialize the spider</p></li>
<li><p><strong>kwargs</strong> (<em>dict</em>) – keyword arguments to initialize the spider</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.crawlers">
<em class="property">property </em><code class="sig-name descname">crawlers</code><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.create_crawler">
<code class="sig-name descname">create_crawler</code><span class="sig-paren">(</span><em class="sig-param">crawler_or_spidercls</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li><p>If <cite>crawler_or_spidercls</cite> is a Crawler, it is returned as-is.</p></li>
<li><p>If <cite>crawler_or_spidercls</cite> is a Spider subclass, a new Crawler
is constructed for it.</p></li>
<li><p>If <cite>crawler_or_spidercls</cite> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</p></li>
</ul>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.join">
<code class="sig-name descname">join</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawlers" title="scrapy.crawler.CrawlerProcess.crawlers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.start">
<code class="sig-name descname">start</code><span class="sig-paren">(</span><em class="sig-param">stop_after_crawl=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.start" title="Permalink to this definition">¶</a></dt>
<dd><p>This method starts a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>, adjusts its pool size to
<a class="reference internal" href="settings.html#std:setting-REACTOR_THREADPOOL_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, and installs a DNS cache based
on <a class="reference internal" href="settings.html#std:setting-DNSCACHE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_ENABLED</span></code></a> and <a class="reference internal" href="settings.html#std:setting-DNSCACHE_SIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_SIZE</span></code></a>.</p>
<p>If <cite>stop_after_crawl</cite> is True, the reactor will be stopped after all
crawlers have finished, using <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.join" title="scrapy.crawler.CrawlerProcess.join"><code class="xref py py-meth docutils literal notranslate"><span class="pre">join()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stop_after_crawl</strong> (<em>boolean</em>) – stop or not the reactor when all
crawlers have finished</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.stop">
<code class="sig-name descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.settings">
<span id="settings-api"></span><span id="topics-api-settings"></span><h2>Settings API<a class="headerlink" href="#module-scrapy.settings" title="Permalink to this headline">¶</a></h2>
<dl class="attribute">
<dt id="scrapy.settings.SETTINGS_PRIORITIES">
<code class="sig-prename descclassname">scrapy.settings.</code><code class="sig-name descname">SETTINGS_PRIORITIES</code><a class="headerlink" href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a></dt>
<dd><p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;default&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;command&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s1">&#39;project&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s1">&#39;spider&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s1">&#39;cmdline&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For a detailed explanation on each settings sources, see:
<a class="reference internal" href="settings.html#topics-settings"><span class="std std-ref">设置</span></a>.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.settings.get_settings_priority">
<code class="sig-prename descclassname">scrapy.settings.</code><code class="sig-name descname">get_settings_priority</code><span class="sig-paren">(</span><em class="sig-param">priority</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.get_settings_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Small helper function that looks up a given string priority in the
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> dictionary and returns its
numerical value, or directly returns a given numerical priority.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.Settings">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.settings.</code><code class="sig-name descname">Settings</code><span class="sig-paren">(</span><em class="sig-param">values=None</em>, <em class="sig-param">priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings</span></code></a></p>
<p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>It is a direct subclass and supports all methods of
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>. Additionally, after instantiation
of this class, the new object will have the global default settings
described on <a class="reference internal" href="settings.html#topics-settings-ref"><span class="std std-ref">内置设置参考</span></a> already populated.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.BaseSettings">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.settings.</code><code class="sig-name descname">BaseSettings</code><span class="sig-paren">(</span><em class="sig-param">values=None</em>, <em class="sig-param">priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instances of this class behave like dictionaries, but store priorities
along with their <code class="docutils literal notranslate"><span class="pre">(key,</span> <span class="pre">value)</span></code> pairs, and can be frozen (i.e. marked
immutable).</p>
<p>Key-value entries can be passed on initialization with the <code class="docutils literal notranslate"><span class="pre">values</span></code>
argument, and they would take the <code class="docutils literal notranslate"><span class="pre">priority</span></code> level (unless <code class="docutils literal notranslate"><span class="pre">values</span></code> is
already an instance of <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>, in which
case the existing priority levels will be kept).  If the <code class="docutils literal notranslate"><span class="pre">priority</span></code>
argument is a string, the priority name will be looked up in
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a>. Otherwise, a specific integer
should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> method, and can be accessed with
the square bracket notation of dictionaries, or with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a> method of the instance and its
value conversion variants. When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<dl class="method">
<dt id="scrapy.settings.BaseSettings.copy">
<code class="sig-name descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a deep copy of current settings.</p>
<p>This method returns a new instance of the <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class,
populated with the same values and their priorities.</p>
<p>Modifications to the new object won’t be reflected on the original
settings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.copy_to_dict">
<code class="sig-name descname">copy_to_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy_to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a copy of current settings and convert to a dict.</p>
<p>This method returns a new dict populated with the same values
and their priorities as the current settings.</p>
<p>Modifications to the returned dict won’t be reflected on the original
settings.</p>
<p>This method can be useful for example for printing settings
in Scrapy shell.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.freeze">
<code class="sig-name descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable further changes to the current settings.</p>
<p>After calling this method, the present state of the settings will become
immutable. Trying to change values through the <a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> method and
its variants won’t be possible and will be alerted.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.frozencopy">
<code class="sig-name descname">frozencopy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.frozencopy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an immutable copy of the current settings.</p>
<p>Alias for a <a class="reference internal" href="#scrapy.settings.BaseSettings.freeze" title="scrapy.settings.BaseSettings.freeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">freeze()</span></code></a> call in the object returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.copy" title="scrapy.settings.BaseSettings.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.get">
<code class="sig-name descname">get</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value without affecting its original type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getbool">
<code class="sig-name descname">getbool</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">default=False</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getbool" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a boolean.</p>
<p><code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">'1'</span></code>, <cite>True`</cite> and <code class="docutils literal notranslate"><span class="pre">'True'</span></code> return <code class="docutils literal notranslate"><span class="pre">True</span></code>,
while <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">'0'</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">'False'</span></code> and <code class="docutils literal notranslate"><span class="pre">None</span></code> return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal notranslate"><span class="pre">'0'</span></code> will return <code class="docutils literal notranslate"><span class="pre">False</span></code> when using this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getdict">
<code class="sig-name descname">getdict</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a dictionary. If the setting original type is a
dictionary, a copy of it will be returned. If it is a string it will be
evaluated as a JSON dictionary. In the case that it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a> instance itself, it will be
converted to a dictionary, containing all its current settings values
as they would be returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a>,
and losing all information about priority and mutability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getfloat">
<code class="sig-name descname">getfloat</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">default=0.0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getfloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a float.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getint">
<code class="sig-name descname">getint</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">default=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getint" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as an int.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getlist">
<code class="sig-name descname">getlist</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a list. If the setting original type is a list, a
copy of it will be returned. If it’s a string it will be split by “,”.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal notranslate"><span class="pre">'one,two'</span></code> will return a list [‘one’, ‘two’] when using this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getpriority">
<code class="sig-name descname">getpriority</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the current numerical priority value of a setting, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if
the given <code class="docutils literal notranslate"><span class="pre">name</span></code> does not exist.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>string</em>) – the setting name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getwithbase">
<code class="sig-name descname">getwithbase</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getwithbase" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a composition of a dictionary-like setting and its <cite>_BASE</cite>
counterpart.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>string</em>) – name of the dictionary-like setting</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.maxpriority">
<code class="sig-name descname">maxpriority</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.maxpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the numerical value of the highest priority present throughout
all settings, or the numerical value for <code class="docutils literal notranslate"><span class="pre">default</span></code> from
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> if there are no settings
stored.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.set">
<code class="sig-name descname">set</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">value</em>, <em class="sig-param">priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.set" title="Permalink to this definition">¶</a></dt>
<dd><p>Store a key/value attribute with a given priority.</p>
<p>Settings should be populated <em>before</em> configuring the Crawler object
(through the <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure()</span></code> method),
otherwise they won’t have any effect.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>value</strong> (<em>any</em>) – the value to associate with the setting</p></li>
<li><p><strong>priority</strong> (<em>string</em><em> or </em><em>int</em>) – the priority of the setting. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.setmodule">
<code class="sig-name descname">setmodule</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.setmodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> for every globally declared
uppercase variable of <code class="docutils literal notranslate"><span class="pre">module</span></code> with the provided <code class="docutils literal notranslate"><span class="pre">priority</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>module object</em><em> or </em><em>string</em>) – the module or the path of the module</p></li>
<li><p><strong>priority</strong> (<em>string</em><em> or </em><em>int</em>) – the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">values</em>, <em class="sig-param">priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> for every item of <code class="docutils literal notranslate"><span class="pre">values</span></code>
with the provided <code class="docutils literal notranslate"><span class="pre">priority</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">values</span></code> is a string, it is assumed to be JSON-encoded and parsed
into a dict with <code class="docutils literal notranslate"><span class="pre">json.loads()</span></code> first. If it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a> instance, the per-key priorities
will be used and the <code class="docutils literal notranslate"><span class="pre">priority</span></code> parameter ignored. This allows
inserting/updating settings with different priorities with a single
command.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> (dict or string or <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>) – the settings names and values</p></li>
<li><p><strong>priority</strong> (<em>string</em><em> or </em><em>int</em>) – the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.loader">
<span id="spiderloader-api"></span><span id="topics-api-spiderloader"></span><h2>SpiderLoader API<a class="headerlink" href="#module-scrapy.loader" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="scrapy.loader.SpiderLoader">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.loader.</code><code class="sig-name descname">SpiderLoader</code><a class="headerlink" href="#scrapy.loader.SpiderLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is in charge of retrieving and handling the spider classes
defined across the project.</p>
<p>Custom spider loaders can be employed by specifying their path in the
<a class="reference internal" href="settings.html#std:setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> project setting. They must fully implement
the <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.interfaces.ISpiderLoader</span></code> interface to guarantee an
errorless execution.</p>
<dl class="method">
<dt id="scrapy.loader.SpiderLoader.from_settings">
<code class="sig-name descname">from_settings</code><span class="sig-paren">(</span><em class="sig-param">settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>This class method is used by Scrapy to create an instance of the class.
It’s called with the current project settings, and it loads the spiders
found recursively in the modules of the <a class="reference internal" href="settings.html#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>
setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>settings</strong> (<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> instance) – project settings</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.SpiderLoader.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param">spider_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Spider class with the given name. It’ll look into the previously
loaded spiders for a spider class with name <cite>spider_name</cite> and will raise
a KeyError if not found.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>spider_name</strong> (<em>str</em>) – spider class name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.SpiderLoader.list">
<code class="sig-name descname">list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the names of the available spiders in the project.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.SpiderLoader.find_by_request">
<code class="sig-name descname">find_by_request</code><span class="sig-paren">(</span><em class="sig-param">request</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.find_by_request" title="Permalink to this definition">¶</a></dt>
<dd><p>List the spiders’ names that can handle the given request. Will try to
match the request’s url against the domains of the spiders.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>request</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instance) – queried request</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.signalmanager">
<span id="signals-api"></span><span id="topics-api-signals"></span><h2>Signals API<a class="headerlink" href="#module-scrapy.signalmanager" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="scrapy.signalmanager.SignalManager">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.signalmanager.</code><code class="sig-name descname">SignalManager</code><span class="sig-paren">(</span><em class="sig-param">sender=_Anonymous</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.signalmanager.SignalManager.connect">
<code class="sig-name descname">connect</code><span class="sig-paren">(</span><em class="sig-param">receiver</em>, <em class="sig-param">signal</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.connect" title="Permalink to this definition">¶</a></dt>
<dd><p>Connect a receiver function to a signal.</p>
<p>The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the <a class="reference internal" href="signals.html#topics-signals"><span class="std std-ref">信号(Signals)</span></a>
section.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>receiver</strong> (<em>callable</em>) – the function to be connected</p></li>
<li><p><strong>signal</strong> (<em>object</em>) – the signal to connect to</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect">
<code class="sig-name descname">disconnect</code><span class="sig-paren">(</span><em class="sig-param">receiver</em>, <em class="sig-param">signal</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect a receiver function from a signal. This has the
opposite effect of the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method, and the arguments
are the same.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect_all">
<code class="sig-name descname">disconnect_all</code><span class="sig-paren">(</span><em class="sig-param">signal</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect all receivers from the given signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>signal</strong> (<em>object</em>) – the signal to disconnect from</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log">
<code class="sig-name descname">send_catch_log</code><span class="sig-paren">(</span><em class="sig-param">signal</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="Permalink to this definition">¶</a></dt>
<dd><p>Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log_deferred">
<code class="sig-name descname">send_catch_log_deferred</code><span class="sig-paren">(</span><em class="sig-param">signal</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log_deferred" title="Permalink to this definition">¶</a></dt>
<dd><p>Like <a class="reference internal" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">send_catch_log()</span></code></a> but supports returning <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">deferreds</a> from
signal handlers.</p>
<p>Returns a Deferred that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stats-collector-api">
<span id="topics-api-stats"></span><h2>Stats Collector API<a class="headerlink" href="#stats-collector-api" title="Permalink to this headline">¶</a></h2>
<p>There are several Stats Collectors available under the
<a class="reference internal" href="stats.html#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.statscollectors</span></code></a> module and they all implement the Stats
Collector API defined by the <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code></a>
class (which they all inherit from).</p>
<span class="target" id="module-scrapy.statscollectors"></span><dl class="class">
<dt id="scrapy.statscollectors.StatsCollector">
<em class="property">class </em><code class="sig-prename descclassname">scrapy.statscollectors.</code><code class="sig-name descname">StatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.StatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.get_value">
<code class="sig-name descname">get_value</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the value for the given stats key or default if it doesn’t exist.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.get_stats">
<code class="sig-name descname">get_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all stats from the currently running spider as a dict.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.set_value">
<code class="sig-name descname">set_value</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given stats key.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.set_stats">
<code class="sig-name descname">set_stats</code><span class="sig-paren">(</span><em class="sig-param">stats</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Override the current stats with the dict passed in <code class="docutils literal notranslate"><span class="pre">stats</span></code> argument.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.inc_value">
<code class="sig-name descname">inc_value</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">count=1</em>, <em class="sig-param">start=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.inc_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Increment the value of the given stats key, by the given count,
assuming the start value given (when it’s not set).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.max_value">
<code class="sig-name descname">max_value</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.max_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.min_value">
<code class="sig-name descname">min_value</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.min_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.clear_stats">
<code class="sig-name descname">clear_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.clear_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all stats.</p>
</dd></dl>

<p>The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:</p>
<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.open_spider">
<code class="sig-name descname">open_spider</code><span class="sig-paren">(</span><em class="sig-param">spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Open the given spider for stats collection.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.close_spider">
<code class="sig-name descname">close_spider</code><span class="sig-paren">(</span><em class="sig-param">spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the given spider. After this is called, no more specific stats
can be accessed or collected.</p>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="signals.html" class="btn btn-neutral float-right" title="信号(Signals)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="extensions.html" class="btn btn-neutral float-left" title="扩展" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008–2018, Scrapy developers
      <span class="lastupdated">
        Last updated on Feb 27, 2020.
      </span>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
 
<script type="text/javascript">
!function(){var analytics=window.analytics=window.analytics||[];if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","page","once","off","on"];analytics.factory=function(t){return function(){var e=Array.prototype.slice.call(arguments);e.unshift(t);analytics.push(e);return analytics}};for(var t=0;t<analytics.methods.length;t++){var e=analytics.methods[t];analytics[e]=analytics.factory(e)}analytics.load=function(t){var e=document.createElement("script");e.type="text/javascript";e.async=!0;e.src=("https:"===document.location.protocol?"https://":"http://")+"cdn.segment.com/analytics.js/v1/"+t+"/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n)};analytics.SNIPPET_VERSION="3.1.0";
analytics.load("8UDQfnf3cyFSTsM4YANnW5sXmgZVILbA");
analytics.page();
}}();

analytics.ready(function () {
    ga('require', 'linker');
    ga('linker:autoLink', ['scrapinghub.com', 'crawlera.com']);
});
</script>


</body>
</html>